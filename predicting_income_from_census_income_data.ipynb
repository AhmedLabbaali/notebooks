{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis of the census income data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "read_csv has all kinds of important tuning parameters which can make the reading and the saving of the data more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"adult.csv\").drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how many classes do we have and if the data is  imbalanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.target.value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View a few rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some statistics for the numerical variables - if you like numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerical_columns = df.describe().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some boxplots - if you like visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(30,5))\n",
    "for i,col in enumerate(numerical_columns):\n",
    "    plt.subplot(1,len(numerical_columns),i+1)\n",
    "    df[[col]].boxplot(fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the most frequent categories for the categorical features - notice missing values (?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def describe_categorical_values(df, non_interesting_columns=[], num_categories=5):\n",
    "    values_df = pd.DataFrame()\n",
    "    for i, column in enumerate(df.columns):\n",
    "        if column in non_interesting_columns:\n",
    "            continue\n",
    "        top_values0 = [\"{}: {}%\".format(x,int(round(100*y/len(df))))\n",
    "                       for x, y in zip(df[column].value_counts(dropna=False).head(num_categories).index,\n",
    "                                       df[column].value_counts(dropna=False).head(num_categories).values)]\n",
    "        if len(top_values0) < num_categories:\n",
    "            top_values = [None]*num_categories\n",
    "            top_values[:len(top_values0)] = top_values0\n",
    "        else:\n",
    "            top_values = top_values0\n",
    "        values_df[column] = top_values\n",
    "    return values_df.transpose()\n",
    "\n",
    "describe_categorical_values(df, non_interesting_columns=numerical_columns, num_categories=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the distributions of the numerical variables per class using KDE (Kernel Density Estimation) plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "for col in numerical_columns:\n",
    "    plt.figure(figsize=(30,3))\n",
    "    sns.kdeplot(df.loc[df.target=='<=50K',col], label='<=50K')\n",
    "    sns.kdeplot(df.loc[df.target=='>50K',col], label='>50K')\n",
    "    plt.legend(fontsize=22)\n",
    "    plt.title(col, fontsize=22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KDE plots doesn't always work so great, especially for integers - let's try bubble plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bubble plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def get_point(x, digits=2):\n",
    "    a, b = x.split(',')\n",
    "    a = float(a.strip(\"(\"))\n",
    "    b = float(b.strip(\"]\"))\n",
    "    c = (a+b)/2\n",
    "    return np.round(c,digits)\n",
    "\n",
    "\n",
    "def bubble_plot(df, x, y, z_boolean=None, ordered_x_values=None, ordered_y_values=None, bins_x=10,\n",
    "                bins_y=10, fontsize=16, figsize=(10,5), maximal_bubble_size=4000,\n",
    "                normalization_by_all = False, log=False):\n",
    "    \"\"\"\n",
    "    :param df: dataframe\n",
    "    :param x:  name of first numerical/categorical field (string) (for x-axis)\n",
    "    :param y: name of second numerical/categorical field (string) (for y-axis)\n",
    "    :param z_boolean: name of categorical field with two categories / boolean field (for coloring)\n",
    "    :param ordered_x_values: the values we would like to map from x categorical variable \n",
    "    according to the order we would like to present them\n",
    "    :param ordered_y_values: the values we would like to map from the y categorical variable \n",
    "    according to the order we would like to present them\n",
    "    :param bins_x: the bins for x values if x is numberic\n",
    "    :param bins_y: the bins for y values if y is numberic\n",
    "    :param normalization_by_all: True - shows joint distribution p(x,y), False - shows conditional distribution p(y|x)\n",
    "    :param maximal_bubble_size: if the bubbles are too big or too small this is the parameter you should change!\n",
    "    :param log: whether to apply log on the count (influence the size of the bubbles)\n",
    "    :return: nice bubble plot, bubble size is propotional to the frequency of the bucket :)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    x_is_numeric = df[x].dtype in (float, int) and ordered_x_values is None\n",
    "    y_is_numeric = df[y].dtype in (float, int) and ordered_y_values is None \n",
    "    count_table = pd.concat([pd.cut(df[x], bins=bins_x) if x_is_numeric else df[x],\n",
    "                             pd.cut(df[y], bins=bins_y) if y_is_numeric else df[y]], axis=1)\n",
    "    count_table = count_table.groupby(x)[y].value_counts().unstack().fillna(0)\n",
    "    ordered_x_values = count_table.index.values if ordered_x_values is None else ordered_x_values\n",
    "    ordered_y_values = count_table.columns if ordered_y_values is None else ordered_y_values\n",
    "    if z_boolean is not None:\n",
    "        count_table_long, xticks, yticks, xticklabels, yticklabels = plot_with_z(df, x, y, z_boolean, bins_x, bins_y, x_is_numeric, y_is_numeric, ordered_x_values, ordered_y_values, maximal_bubble_size, \n",
    "                                                                                 normalization_by_all=normalization_by_all)\n",
    "    else:\n",
    "        count_table_long, xticks, yticks, xticklabels, yticklabels = plot_without_z(df, x, y, z_boolean, count_table, bins_x, bins_y, x_is_numeric, y_is_numeric, ordered_x_values, ordered_y_values, \n",
    "                                                                                    normalization_by_all=normalization_by_all, log=log, maximal_bubble_size=maximal_bubble_size )\n",
    "    plt.xticks(xticks, xticklabels,fontsize=fontsize)\n",
    "    plt.yticks(yticks, yticklabels,fontsize=fontsize)\n",
    "    plt.xlabel(x, fontsize=fontsize)\n",
    "    plt.ylabel(y, fontsize=fontsize)\n",
    "    if z_boolean is None:\n",
    "        plt.title(\"{} vs {} \".format(y,x),fontsize=fontsize+4);\n",
    "    else:\n",
    "        plt.title(\"{} vs {} and {} (in colors)\".format(y,x, z_boolean),fontsize=fontsize+4);\n",
    "\n",
    "def plot_without_z(df, x, y, z, count_table, bins_x, bins_y, x_is_numeric, y_is_numeric, ordered_x_values, ordered_y_values, normalization_by_all=False, log=False, maximal_bubble_size=4000):\n",
    "    if normalization_by_all:\n",
    "        count_table /= count_table.sum().sum()\n",
    "    else:\n",
    "        count_table = count_table.transpose()\n",
    "        for col in count_table.columns:\n",
    "            count_table[col] /= count_table[col].sum()\n",
    "        count_table = count_table.transpose()\n",
    "    if log:\n",
    "        count_table = np.log(count_table)\n",
    "        maximal_bubble_size /= 2\n",
    "    size_factor = maximal_bubble_size/count_table.max().max()\n",
    "    count_table_long = pd.melt(count_table.reset_index(), id_vars=x)\n",
    "    x_values_dict = {x:i for i, x in enumerate(ordered_x_values)} \\\n",
    "        if not x_is_numeric else {xx:get_point(xx) for xx in ordered_x_values}\n",
    "    y_values_dict = {x:i for i, x in enumerate(ordered_y_values)} \\\n",
    "        if not y_is_numeric else {xx: get_point(xx) for xx in ordered_y_values}\n",
    "    xticks = np.arange(count_table.shape[0]) if not x_is_numeric else [get_point(xx) for xx in ordered_x_values]\n",
    "    yticks = np.arange(count_table.shape[1]) if not y_is_numeric else [get_point(xx) for xx in ordered_y_values]\n",
    "    xticklabels = ordered_x_values if not x_is_numeric else [get_point(xx) for xx in ordered_x_values]\n",
    "    yticklabels = ordered_y_values if not y_is_numeric else [get_point(xx) for xx in ordered_y_values]\n",
    "    count_table_long[x] = count_table_long[x].map(x_values_dict)\n",
    "    count_table_long[y] = count_table_long[y].map(y_values_dict) \n",
    "    plt.scatter(count_table_long[x], count_table_long[y], s=size_factor*count_table_long['value'],\n",
    "                c=count_table_long['value'], cmap='cool')\n",
    " \n",
    "    return count_table_long, xticks, yticks, xticklabels, yticklabels\n",
    "    \n",
    "def plot_with_z(df, x, y, z_boolean, bins_x, bins_y, x_is_numeric, y_is_numeric, ordered_x_values, ordered_y_values, maximal_bubble_size=4000, normalization_by_all=False):\n",
    "    count_table = pd.concat([pd.cut(df[x], bins=bins_x) if x_is_numeric else df[x],\n",
    "                         pd.cut(df[y], bins=bins_y) if y_is_numeric else df[y], df[z_boolean]], axis=1)\n",
    "    count_table = count_table.groupby([x,z_boolean])[y].value_counts().unstack().fillna(0)\n",
    "    count_table = count_table.unstack()\n",
    "    count_table_long = pd.melt(count_table.reset_index(), id_vars=x)\n",
    "    z_boolean_values = count_table_long[z_boolean].unique()\n",
    "    ratio = pd.DataFrame({'ratio':count_table_long.set_index([x,y,z_boolean]).unstack()['value'][z_boolean_values[1]] / (\n",
    "    count_table_long.set_index([x,y,z_boolean]).unstack()['value'].sum(axis=1) )})\n",
    "    count_table_long = count_table_long.set_index([x, y ])[['value']].merge(ratio, left_index=True, right_index=True).reset_index()\n",
    "    size_factor = maximal_bubble_size/count_table_long['value'].max()\n",
    "    x_values_dict = {x:i for i, x in enumerate(ordered_x_values)} \\\n",
    "        if not x_is_numeric else {xx:get_point(xx) for xx in ordered_x_values}\n",
    "    y_values_dict = {x:i for i, x in enumerate(ordered_y_values)} \\\n",
    "        if not y_is_numeric else {xx: get_point(xx) for xx in ordered_y_values}\n",
    "    xticks = np.arange(len(ordered_x_values)) if not x_is_numeric else [get_point(xx) for xx in ordered_x_values]\n",
    "    yticks = np.arange(len(ordered_y_values)) if not y_is_numeric else [get_point(xx) for xx in ordered_y_values]\n",
    "    xticklabels = ordered_x_values if not x_is_numeric else [get_point(xx) for xx in ordered_x_values]\n",
    "    yticklabels = ordered_y_values if not y_is_numeric else [get_point(xx) for xx in ordered_y_values]\n",
    "    count_table_long[x] = count_table_long[x].map(x_values_dict)\n",
    "    count_table_long[y] = count_table_long[y].map(y_values_dict)\n",
    "    plt.scatter(count_table_long[x], count_table_long[y], s=size_factor*count_table_long['value'],\n",
    "                c=count_table_long['ratio'],  alpha=0.5,\n",
    "                cmap='cool')\n",
    "    return count_table_long, xticks, yticks, xticklabels, yticklabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at one variable vs. the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "bubble_plot(df,'age','target', normalization_by_all=False)\n",
    "bubble_plot(df,'hours-per-week','target', normalization_by_all=False)\n",
    "bubble_plot(df,'education-num','target', normalization_by_all=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at two variables vs the target (the color simbols the target ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bubble_plot(df, x='age', y='hours-per-week', z_boolean='target', maximal_bubble_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing values for categorical columns with the most common values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to deal with missing values?**\n",
    "\n",
    "(1) missing at random - imputation - using common values or estimation of the value using regression methods\n",
    "\n",
    "(2) missing not at random - treat missing value as a separate indicator variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.csv\").drop('Unnamed: 0',axis=1)\n",
    "categorical_columns = [x for x in df.columns if x not in numerical_columns and x!='target']\n",
    "for col in categorical_columns:\n",
    "    df.loc[df[col]==' ?',col] = df[col].value_counts().index.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove infrequent categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_infrequent_values(df, categorical_columns, frequency_threshold=20):\n",
    "    infrequent_cols_to_remove = []\n",
    "    for cat_col in categorical_columns:\n",
    "        col_count = df[cat_col].value_counts()\n",
    "        infrequent_values = col_count[col_count<frequency_threshold].index.values\n",
    "        if not col_count[col_count<20].empty:\n",
    "            print(\"removing columns: \\n{} \\n\".format(col_count[col_count<frequency_threshold]))\n",
    "        infrequent_cols_to_remove += [\"{}_{}\".format(cat_col, x) for x in infrequent_values]\n",
    "    return df\n",
    "\n",
    "df = remove_infrequent_values(df, categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert categorical variables to dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.csv\").drop('Unnamed: 0',axis=1)\n",
    "y = df.target==\">50K\"\n",
    "features = [x for x in df.columns if x!='target']\n",
    "X = pd.get_dummies(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split to train and test data - using StratifiedKFold - to keep the target distribution in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "kf = StratifiedKFold(y, 5, random_state=12345, shuffle=True)\n",
    "train_index, test_index = list(kf)[0]\n",
    "train, test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.target.value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check correlation between the variables and between the variables and the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important specifically for Logistic regression where correlation between variables can contribute to a numerically unstable solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_most_correlated_variables(corr, num_pairs=10):\n",
    "    correlation_melted = pd.melt(corr.reset_index().rename(columns={\"index\": \"var_1\"}), id_vars=(\"var_1\"),var_name='var_2')\n",
    "    correlation_melted = correlation_melted[correlation_melted.var_1!=correlation_melted.var_2]\n",
    "    correlation_melted['var_couple'] = correlation_melted[['var_1','var_2']].apply(lambda x:tuple(sorted([x[0],x[1]])), axis=1)\n",
    "    correlation_melted = correlation_melted.drop_duplicates(subset='var_couple').drop(['var_couple'],axis=1)\n",
    "    correlation_melted['abs_value'] = correlation_melted['value'].abs().round(3)\n",
    "    return correlation_melted.sort_values(by='abs_value').tail(num_pairs).drop('abs_value', axis=1).reset_index(drop=True)\n",
    "\n",
    "def plot_correlation_matrix(X, features2):\n",
    "    corr = X[features2].corr()\n",
    "     # return the most correlated variables\n",
    "    most_correlated_variables = get_most_correlated_variables(corr, num_pairs=10)\n",
    "    max_correlation = 1.25*most_correlated_variables['value'].abs().max()\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    ax.set_yticklabels(features2, fontsize=18)\n",
    "    ax.set_xticklabels(features2, rotation='vertical', fontsize=18)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=max_correlation, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "    return most_correlated_variables\n",
    "if 'target' not in X:\n",
    "    X['target'] = y\n",
    "features2 = ['target'] + X.columns[:20].tolist()\n",
    "plot_correlation_matrix(X, features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features2 =['target']+[x for x in X.columns if x.startswith('education') ]\n",
    "plot_correlation_matrix(X, features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features2 =['target']+[x for x in X.columns if x.startswith('relationship_') or x.startswith('marital')]\n",
    "plot_correlation_matrix(X, features2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find features which are highly correlated to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'target' in X:\n",
    "    X = X.drop('target', axis=1)\n",
    "corr = X.corr()\n",
    "most_correlated_variables = get_most_correlated_variables(corr, num_pairs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_correlated_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_correlated_variables.tail(5).var_2.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find features which are highly correlated with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'target' not in X:\n",
    "    X['target'] = y\n",
    "corr = X.corr()\n",
    "corr_with_target = corr[['target']].drop('target', axis=0)\n",
    "corr_with_target['abs_value'] = corr_with_target.abs()\n",
    "corr_with_target.sort_values(by='abs_value').tail(10).drop('abs_value',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features_by_correlation = corr_with_target[corr_with_target.abs_value>0.05].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(best_features_by_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction with PCA \n",
    "Data is projected into a new space where the new transformed features has no mutual correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=train.shape[1])\n",
    "train_pca = pca.fit_transform(train, )\n",
    "test_pca = pca.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(train_pca[(y_train==0).values,0], train_pca[(y_train==0).values,1], color='r', label='>50K')\n",
    "plt.scatter(train_pca[(y_train==1).values,0], train_pca[(y_train==1).values,1], color='b', label='<=50K')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Explained variance in each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,5))\n",
    "pd.DataFrame(pca.explained_variance_ratio_).plot.bar(log=False, figsize=(35,7), fontsize=16)\n",
    "plt.xlabel('n_components',fontsize=16)\n",
    "plt.ylabel('explained_variance_', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,5))\n",
    "pd.DataFrame(pca.explained_variance_ratio_).plot.bar(log=True, figsize=(35,7), fontsize=16)\n",
    "plt.xlabel('n_components',fontsize=16)\n",
    "plt.ylabel('explained_variance_', fontsize=16);\n",
    "plt.ylim([0,10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic regression model and evaluate its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "\n",
    "def find_best_threshold(thresholds, fpr, tpr):\n",
    "    \"\"\"\n",
    "    find the best threshold from the roc curve. by finding the threshold for the point which is closest to (fpr=0,tpr=1)\n",
    "    \"\"\"\n",
    "    fpr_tpr = pd.DataFrame({'thresholds': thresholds, 'fpr': fpr, 'tpr': tpr})\n",
    "    fpr_tpr['dist'] = (fpr_tpr['fpr'])**2 + (fpr_tpr['tpr']-1)**2\n",
    "    return fpr_tpr.ix[fpr_tpr.dist.idxmin(), 'thresholds']\n",
    "\n",
    "\n",
    "def get_model_results(model, train, test, y_train, y_test):\n",
    "    probabilities = model.predict_proba(test)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "    threshold = find_best_threshold(thresholds, fpr, tpr)\n",
    "    predictions = probabilities>threshold\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='test')\n",
    "    roc_auc = roc_auc_score(y_test, probabilities)\n",
    "    probabilities = model.predict_proba(train)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, probabilities)\n",
    "    plt.plot(fpr, tpr, label='train')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='random guess')\n",
    "    plt.title(\"area under the ROC curve = {}\".format(roc_auc), fontsize=18);\n",
    "    print(classification_report(y_test, predictions))\n",
    "    plt.legend()\n",
    "\n",
    "model = make_pipeline(LogisticRegression())\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(MinMaxScaler(),LogisticRegression())\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(PCA(n_components=30,whiten=False),LogisticRegression())\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(),MinMaxScaler(), LogisticRegression())\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with Polynomial features only for the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "train_poly_features = poly.fit_transform(train[numerical_columns])\n",
    "test_poly_features = poly.transform(test[numerical_columns])\n",
    "train_with_poly_features = pd.concat([train.reset_index(drop=True),pd.DataFrame(train_poly_features)],axis=1)\n",
    "test_with_poly_features = pd.concat([test.reset_index(drop=True),pd.DataFrame(test_poly_features)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(MinMaxScaler(),LogisticRegression())\n",
    "model.fit(train_with_poly_features, y_train)\n",
    "get_model_results(model, train_with_poly_features, test_with_poly_features, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the logistic regression coefficients"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "best_features = feature_importance.sort_values(by='abs_value')['coef'].tail(50).index.values\n",
    "model = make_pipeline(MinMaxScaler(),LogisticRegression())\n",
    "model.fit(train_with_poly_features[best_features], y_train)\n",
    "get_model_results(model, train_with_poly_features[best_features], test_with_poly_features[best_features], y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "params = {#'logisticregression__penalty': ['l2','l1'], 'logisticregression__C': [1, 10, 100, 1000],\n",
    "          'logisticregression__penalty': ['l1'], 'logisticregression__C': [1,],\n",
    "#                  'solver': ['newton-cg'],\n",
    "#                  'logisticregression__fit_intercept': [False, True]\n",
    "         }\n",
    "gscv = GridSearchCV(make_pipeline(MinMaxScaler(), LogisticRegression()), params, scoring='roc_auc', cv=3)\n",
    "gscv.fit(train, y_train)\n",
    "get_model_results(gscv, train, test, y_train, y_test)\n",
    "print(gscv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=3, n_estimators=100) # max_depth=4\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot one of the random forest trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from os import system\n",
    "\n",
    "tree_dot_path = \"models/tree1.dot\"\n",
    "tree_png_path = \"models/tree1.png\"\n",
    "dotfile = open(tree_dot_path, 'w')\n",
    "tree.export_graphviz(model.estimators_[10], out_file = dotfile, \n",
    "                     feature_names = train.columns, \n",
    "                     filled=True)\n",
    "dotfile.close()\n",
    "system(\"dot -Tpng {tree_dot_path} -o {tree_png_path}\".format(\n",
    "        tree_dot_path=tree_dot_path, tree_png_path=tree_png_path));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=models/tree1.png width=\"1700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Extreme gradient boosted trees\n",
    "https://github.com/dmlc/xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(train_with_poly_features, y_train)\n",
    "get_model_results(model, train_with_poly_features, test_with_poly_features, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importnace for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "feature_importance = pd.Series(index=train.columns , data= model.feature_importances_)\n",
    "feature_importance = feature_importance.sort_values()\n",
    "feature_importance.tail(20).plot.barh(fontsize=16, figsize=(14,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost classifier with the 15 strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_features = feature_importance.tail(15).index.values\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(train[best_features].rename(columns={x:x.replace(\" \",\"\") for x in best_features}), y_train)\n",
    "\n",
    "get_model_results(model, train[best_features].rename(columns={x:x.replace(\" \",\"\") for x in best_features}),  test[best_features].rename(columns={x:x.replace(\" \",\"\") for x in best_features}), y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot one of the xgboost trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from xgboost import plot_tree\n",
    "from matplotlib.pylab import rcParams\n",
    "ax = plot_tree(model,num_trees=10, rankdir='LR')\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(20, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the data with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE() #n_iter=200)\n",
    "# train_transformed = tsne.fit_transform(train[best_features])\n",
    "# pd.DataFrame(train_transformed).to_pickle(\"train_transformed.pkl\")\n",
    "train_transformed = pd.read_pickle(\"train_transformed.pkl\").values\n",
    "plt.scatter(train_transformed[(y_train==0).values,0], train_transformed[(y_train==0).values,1], color='r', label='class 1')\n",
    "plt.scatter(train_transformed[(y_train==1).values,0], train_transformed[(y_train==1).values,1], color='b', label='class 2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: try encoding the variables based on conditional distribution P(y=1 / x=xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_column_by_frequency(train, test, col, log=True):\n",
    "    country_by_target = train.groupby(col)['target'].value_counts().unstack().fillna(0)\n",
    "    if log:\n",
    "        country_probability = np.log((country_by_target['>50K']+1e-5)/(country_by_target.sum(axis=1)+1e-5))\n",
    "    else:\n",
    "        country_probability = (country_by_target['>50K']+1e-5)/(country_by_target.sum(axis=1)+1e-5)\n",
    "    for val in test[col].unique():    \n",
    "        if val in country_probability.index.values: \n",
    "            continue\n",
    "        country_probability[val] = country_probability.median()\n",
    "    train[col] = train[col].map(country_probability)\n",
    "    test[col] = test[col].map(country_probability)\n",
    "    return train, test\n",
    "\n",
    "def encode_numeric_column_by_frequency(train, test, col, log=True, plot_flag=False, window_size=3):\n",
    "    country_by_target = train.groupby(col)['target'].value_counts().unstack().fillna(0)\n",
    "    if log:\n",
    "        country_probability = np.log((country_by_target['>50K']+1e-5)/(country_by_target.sum(axis=1)+1e-5))\n",
    "    else:\n",
    "        country_probability = (country_by_target['>50K']+1e-5)/(country_by_target.sum(axis=1)+1e-5)\n",
    "    values = np.unique(train[col].unique().tolist()+test[col].unique().tolist())\n",
    "    resolution = min(1,np.min(np.diff(values)))\n",
    "    start = np.min(country_probability.index.unique())\n",
    "    end = np.max(country_probability.index.unique())\n",
    "    country_probability = pd.merge(pd.DataFrame(country_probability),\n",
    "                                   pd.DataFrame(index=np.arange(start, end+resolution, resolution)),\n",
    "                                   left_index=True, right_index=True, how='outer').interpolate(method='linear')\n",
    "    if plot_flag:\n",
    "        plt.figure()\n",
    "        country_probability[0].plot(marker='o')\n",
    "    country_probability=((country_probability[0].cumsum() - country_probability[0].shift(window_size).cumsum())/window_size).shift(-1).fillna(0)\n",
    "    if plot_flag:\n",
    "        country_probability.plot()\n",
    "        plt.title(col)\n",
    "    train[col] = train[col].map(country_probability)\n",
    "    test[col] = test[col].map(country_probability)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_with_encoded_fetures(plot_flag=False):\n",
    "    df = pd.read_csv(\"adult.csv\").drop('Unnamed: 0',axis=1)#.drop('fnlwgt', axis=1)\n",
    "    categorical_columns = [x for x in df.columns if x not in numerical_columns and x!='target']\n",
    "    columns_to_encode_with_frequencies=categorical_columns\n",
    "    for col in categorical_columns:\n",
    "        df.loc[df[col]==' ?',col] = df[col].value_counts().index.values[0]\n",
    "    features = [x for x in df if x != 'target']\n",
    "    X = df[features]\n",
    "    y = df.target==\">50K\"\n",
    "    kf = StratifiedKFold(y, 5, random_state=12345, shuffle=True)\n",
    "    train_index, test_index = list(kf)[0]\n",
    "    train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    for col in categorical_columns:\n",
    "        train, test = encode_column_by_frequency(train, test, col, log=False)\n",
    "    for col in numerical_columns: \n",
    "        if col=='fnlwgt': continue\n",
    "        train, test = encode_numeric_column_by_frequency(train, test, col,log=False, plot_flag=plot_flag)\n",
    "    train=train.drop('target',axis=1)\n",
    "    test=test.drop('target',axis=1)\n",
    "    return train, test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with features replaced with conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train, test, y_train, y_test = get_data_with_encoded_fetures(plot_flag=False)\n",
    "model = make_pipeline(MinMaxScaler(),LogisticRegression())\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with features replaced with conditional probabilities + Polynomial features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(), MinMaxScaler(),LogisticRegression())\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST  with features replaced with conditional probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST with features replaced with conditional probabilities + Polynomial features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(), XGBClassifier())\n",
    "model.fit(train, y_train)\n",
    "get_model_results(model, train, test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "- Undetstand which further cool features are missing - use different transformations on the features, use combinations of features, find more data sources which could help to the problem...\n",
    "- Analyze your model errors and feature importance\n",
    "- Use tools like LIME to understand your model decisions and errors\n",
    "- Further tuning of the model - model selection, ensembling and stacking models, hyper-parameters tuning\n",
    "- Present the results in a clear way according to the relevant business metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='meme1.jpg' width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional questions - your'e welcome to stay in touch - meir.shir86@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
